{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task Overview"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "running a project from the start, including \n",
    "- Data splitting\n",
    "- writing a custom data set\n",
    "- training using Finetuning\n",
    "- evaluation of the results\n",
    "\n",
    "What to consider before starting to train\n",
    " - to split the data into training, validation and testing\n",
    " - to verify that your splits are disjoint after creating them\n",
    " - to set up the code so that it can run when the split is re-generated\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you use MonuSEG, then reserve for the test set some tissue types\n",
    "which are not included in train and val. Note: the test set should\n",
    "also contain tissue types included in train and val - for comparison.\n",
    "This allows to compare generalization to unseen tissue types!!\n",
    "\n",
    "evaluate in an **accuracy-type** measure (e.g. mIoU for segmentation) and\n",
    "in a second category.\n",
    "\n",
    "**mIOU** = mean intersection over union"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deliverables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **training phase**: code for training on the dataset with finetuning\n",
    "\n",
    "2. a chosen trained model\n",
    "\n",
    "3. **validation phase**: code which uses the trained model to predict on the test\n",
    "set images and saves those predictions, and which computes the accuracytype evaluation.\n",
    "\n",
    "4. **a reproduction routine**: the scores from the pretrained model\n",
    "computed on the fly when we run your code should be compared\n",
    "against the scores which you saved when you ran your code - for\n",
    "the accuracy type evaluation\n",
    "\n",
    "5. code and results for the second category evaluation\n",
    "\n",
    "6. curves showing for every epoch loss and performance on validation and\n",
    "training set - for the run which generates the model you saved (you could\n",
    "have tried multiple runs with different hyperparameters)\n",
    "\n",
    "7. a brief pdf-report about the above (first and second category evaluation:\n",
    "description and results, the curves) and\n",
    "– your name and your matriculation number\n",
    "– describes the experimental parameters of the training (learning rate\n",
    "batch size, seed values and anything else necessary to reproduce the\n",
    "training)\n",
    "– novels longer than 10 pages will not be entertained.\n",
    "\n",
    "8. put everything: codes, saved model, saved predictions, the pdf and everything else you want to add into one single zip file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extra Binder shit\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **average precision computation:**\n",
    "\n",
    "-you *must* *not* binarize the predictions for any ranking measures\n",
    "-average precision should be computed over the set of all collected predictions for one model and one class.\n",
    "\n",
    "it is not computed over one minibatch  or over a single data point. it is not concatenated over cv folds (unless one would take care to normalize output statistics)\n",
    "\n",
    "2. not an error, but losing lots of predictive accuracy:\n",
    "\n",
    "for a dataset with very different statistics one should train all layers (unless you have like 100 data points in total). I have seen it in at least one pytorch code and in a few keras codes that students train only the last layer. one loses 10-15% acc (if the classifier is a good one, no loss if the classifier predicts random scores, lol )\n",
    "\n",
    "3. mean subtraction and normalization anyone ??                 \n",
    "\n",
    "4. PROJECT SHOULD BE IN PYTORCH.  Was that not self-understanding from the announcement in lecture 1 ??\n",
    "\n",
    "5. randomized transforms (random rotations, color jitter ) are not used for test performance evaluation UNLESS one would average over an application of a few of them on the same data point.  One use those exclusively in training. For validation it is debatable whether to use them or not, but usually one would not use them.\n",
    "\n",
    "6. not an error but questionable practice:\n",
    "\n",
    "doing the train/val/test split on the fly [no deduction] - \n",
    "\n",
    "this might lose information about what part of data was used/not used in training.\n",
    "\n",
    "evaluation can become unreliable if data is split on the fly and the seed is changed or the generator is changed (or possibly the hardware/OS) and you do not know which data samples have been withheld from training x years ago. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pyspark",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
